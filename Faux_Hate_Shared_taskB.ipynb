{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I0X9zmADFOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2250420e-6148-4df1-e952-8936a21d28e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Remove N/A rows from training set**"
      ],
      "metadata": {
        "id": "iL0ZHn2-YLez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# File paths\n",
        "input_file_path = \"/content/drive/MyDrive/Colab Notebooks/shared_task/Train_Task_B.xlsx\"  # Replace with your file path\n",
        "output_file_path_na = \"/content/drive/MyDrive/Colab Notebooks/shared_task/filtered_data.xlsx\"  # File to save rows with N/A and Hate=0\n",
        "output_file_path_remaining = \"/content/drive/MyDrive/Colab Notebooks/shared_task/removed_data.xlsx\"  # File to save remaining rows\n",
        "\n",
        "# Load the Excel file into a Pandas DataFrame\n",
        "df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Ensure \"N/A\" and missing values (NaN) are uniformly treated\n",
        "df = df.fillna(\"N/A\")  # Replace NaN with \"N/A\" if necessary\n",
        "\n",
        "# Display the first few rows for verification (optional)\n",
        "print(\"Initial DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Filter rows where Target and Severity are \"N/A\" and Hate is 0\n",
        "na_condition = (df['Target'] == 'N/A') & (df['Severity'] == 'N/A') & (df['Hate'] == 0)\n",
        "na_rows = df[na_condition]\n",
        "\n",
        "# Filter remaining rows\n",
        "remaining_rows = df[~na_condition]\n",
        "\n",
        "# Save the filtered DataFrames to separate Excel files\n",
        "na_rows.to_excel(output_file_path_na, index=False)\n",
        "remaining_rows.to_excel(output_file_path_remaining, index=False)\n",
        "\n",
        "# Confirm the operation\n",
        "print(f\"Rows with 'N/A' and 'Hate=0' saved to: {output_file_path_na}\")\n",
        "print(f\"Remaining rows saved to: {output_file_path_remaining}\")\n"
      ],
      "metadata": {
        "id": "Td3kttZz1aOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# File paths\n",
        "input_file_path = \"/content/drive/MyDrive/Colab Notebooks/shared_task/Val_Task_B.xlsx\"  # Replace with your file path\n",
        "output_file_path_na = \"/content/drive/MyDrive/Colab Notebooks/shared_task/val_filtered_data.xlsx\"  # File to save rows with N/A and Hate=0\n",
        "output_file_path_remaining = \"/content/drive/MyDrive/Colab Notebooks/shared_task/val_removed_data.xlsx\"  # File to save remaining rows\n",
        "\n",
        "# Load the Excel file into a Pandas DataFrame\n",
        "df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Ensure \"N/A\" and missing values (NaN) are uniformly treated\n",
        "df = df.fillna(\"N/A\")  # Replace NaN with \"N/A\" if necessary\n",
        "\n",
        "# Display the first few rows for verification (optional)\n",
        "print(\"Initial DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Filter rows where Target and Severity are \"N/A\" and Hate is 0\n",
        "na_condition = (df['Target'] == 'N/A') & (df['Severity'] == 'N/A')\n",
        "na_rows = df[na_condition]\n",
        "\n",
        "# Filter remaining rows\n",
        "remaining_rows = df[~na_condition]\n",
        "\n",
        "# Save the filtered DataFrames to separate Excel files\n",
        "na_rows.to_excel(output_file_path_na, index=False)\n",
        "remaining_rows.to_excel(output_file_path_remaining, index=False)\n",
        "\n",
        "# Confirm the operation\n",
        "print(f\"Rows with 'N/A' and 'Hate=0' saved to: {output_file_path_na}\")\n",
        "print(f\"Remaining rows saved to: {output_file_path_remaining}\")\n"
      ],
      "metadata": {
        "id": "n0ruFqTJXl_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove rows from test set where hate = **0**\n"
      ],
      "metadata": {
        "id": "RH2s1fHUDmh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "input_file_path = \"/content/drive/MyDrive/Colab Notebooks/shared_task/task_a_test_predictions.csv\"  # Replace with your file path\n",
        "output_file_path_na = \"/content/drive/MyDrive/Colab Notebooks/shared_task/test_filtered_data.xlsx\"  # File to save rows with Target or Severity = \"N/A\" and Hate=0\n",
        "output_file_path_remaining = \"/content/drive/MyDrive/Colab Notebooks/shared_task/test_removed_data.xlsx\"  # File to save remaining rows\n",
        "\n",
        "# Load the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(input_file_path)\n",
        "\n",
        "# Ensure \"N/A\" and missing values (NaN) are uniformly treated\n",
        "df.fillna(\"N/A\", inplace=True)  # Replace NaN with \"N/A\" if necessary\n",
        "\n",
        "# Display the first few rows for verification (optional)\n",
        "print(\"Initial DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Filter rows where Target or Severity is \"N/A\" and Predicted_Hate is 0\n",
        "na_condition = ((df['Predicted_Hate'] == 0))\n",
        "na_rows = df[na_condition]\n",
        "\n",
        "# Filter remaining rows (complement of the above condition)\n",
        "remaining_rows = df[~na_condition]\n",
        "\n",
        "# Save the filtered DataFrames to separate Excel files\n",
        "na_rows.to_excel(output_file_path_na, index=False)\n",
        "remaining_rows.to_excel(output_file_path_remaining, index=False)\n",
        "\n",
        "# Confirm the operation\n",
        "print(f\"Rows with 'Target or Severity = N/A' and 'Hate=0' saved to: {output_file_path_na}\")\n",
        "print(f\"Remaining rows saved to: {output_file_path_remaining}\")\n"
      ],
      "metadata": {
        "id": "yXAoBKhrDlIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX5__wsiDuF1"
      },
      "outputs": [],
      "source": [
        "#train_a = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/shared_task/Train_Task_A.xlsx')  # Use read_excel for .xlsx files\n",
        "train_set  = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/shared_task/removed_data.xlsx')  # Use read_excel for .xlsx files\n",
        "#val_a = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/shared_task/Val_Task_A.xlsx')  # Use read_excel for .xlsx files\n",
        "val_set = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/shared_task/val_removed_data.xlsx')  # Use read_excel for .xlsx files\n",
        "test_set = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/shared_task/test_removed_data.xlsx')  # Use read_excel for .xlsx files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz3Gan_EDmiL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def text_cleaner(text):\n",
        "    # Check if text is a string before applying lower()\n",
        "    if isinstance(text, str):\n",
        "        #converting to lowercase\n",
        "        newString = text.lower()\n",
        "        #removing links\n",
        "        newString = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', newString)\n",
        "        #fetching alphabetic characters\n",
        "        newString = re.sub('[^a-zA-Z#@]', ' ', newString)\n",
        "        return newString\n",
        "    else:\n",
        "        # Handle non-string values (e.g., return empty string or original value)\n",
        "        return str(text)  # Or return '' if you want to ignore non-string values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceMVzzilPpZJ"
      },
      "outputs": [],
      "source": [
        "train_set['Tweet'] = train_set['Tweet'].apply(text_cleaner)\n",
        "#train_b['Tweet'] = train_b['Tweet'].apply(text_cleaner)\n",
        "val_set['Tweet'] = val_set['Tweet'].apply(text_cleaner)\n",
        "test_set['Tweet'] = test_set['Tweet'].apply(text_cleaner)\n",
        "#val_b\n",
        "# Verify the sizes of the splits\n",
        "print(f\"Train set size: {len(train_set)}\")\n",
        "print(f\"Validation set size: {len(val_set)}\")\n",
        "print(f\"Testing set size: {len(test_set)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assume train_set is a DataFrame with columns ['Hate', 'Target', 'Fake']\n",
        "label_counts = train_set[['Target','Severity']].astype(str).apply(pd.Series.value_counts)\n",
        "print(label_counts)"
      ],
      "metadata": {
        "id": "2kW9_6CMSf8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assume train_set is a DataFrame with columns ['Hate', 'Fake']\n",
        "label_counts = val_set[['Target', 'Severity']].apply(pd.Series.value_counts)\n",
        "print(label_counts)"
      ],
      "metadata": {
        "id": "gb4lQw2QZkFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume train_set is a DataFrame with columns ['Hate', 'Fake']\n",
        "label_counts = test_set[['Target', 'Severity']].apply(pd.Series.value_counts)\n",
        "print(label_counts)"
      ],
      "metadata": {
        "id": "cwBmHq0Xwoc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bz9NEavR68M"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Load the pre-trained tokenizer (HingRoBERTa)\n",
        "model_name = 'l3cube-pune/hing-roberta'  # Replace with desired model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load Task B data (replace with actual file paths)\n",
        "#train_set = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/shared_task/removed_data.xlsx\")\n",
        "#val_set = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/shared_task/val_removed_data.xlsx\")\n",
        "\n",
        "# Extract relevant columns\n",
        "train_texts = train_set['Tweet'].tolist()\n",
        "train_labels = train_set[['Target', 'Severity']].values\n",
        "test_texts = val_set['Tweet'].tolist()\n",
        "test_labels = val_set[['Target', 'Severity']].values\n",
        "\n",
        "# Encode Target and Severity labels\n",
        "target_mapping = {'I': 0, 'O': 1, 'R': 2}  # Map Target classes to integers\n",
        "severity_mapping = {'L': 0, 'M': 1, 'H': 2}  # Map Severity classes to integers\n",
        "\n",
        "train_labels = [[target_mapping[t], severity_mapping[s]] for t, s in train_labels]\n",
        "test_labels = [[target_mapping[t], severity_mapping[s]] for t, s in test_labels]\n",
        "\n",
        "# Compute class weights for both labels\n",
        "class_weights = {}\n",
        "for i, label_name in enumerate(['Target', 'Severity']):\n",
        "    class_weights[label_name] = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique([label[i] for label in train_labels]),\n",
        "        y=[label[i] for label in train_labels]\n",
        "    )\n",
        "\n",
        "# Convert weights to tensors for PyTorch\n",
        "target_class_weights = torch.tensor(class_weights['Target'], dtype=torch.float)\n",
        "severity_class_weights = torch.tensor(class_weights['Severity'], dtype=torch.float)\n",
        "\n",
        "class MultiLabelTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None): # Add num_items_in_batch argument\n",
        "        labels = inputs.pop('labels')  # Extract labels\n",
        "        outputs = model(**inputs)  # Forward pass\n",
        "        logits = outputs.logits\n",
        "        # Split logits into target and severity\n",
        "        target_logits, severity_logits = logits[:, :3], logits[:, 3:]\n",
        "        # Define loss functions\n",
        "        target_loss_fct = torch.nn.CrossEntropyLoss(weight=target_class_weights.to(logits.device))\n",
        "        severity_loss_fct = torch.nn.CrossEntropyLoss(weight=severity_class_weights.to(logits.device))\n",
        "        # Compute losses\n",
        "        target_loss = target_loss_fct(target_logits, labels[:, 0])\n",
        "        severity_loss = severity_loss_fct(severity_logits, labels[:, 1])\n",
        "        # Combine losses\n",
        "        loss = target_loss + severity_loss\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_data = Dataset.from_dict({'text': train_texts, 'labels': train_labels})\n",
        "test_data = Dataset.from_dict({'text': test_texts, 'labels': test_labels})\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
        "\n",
        "train_data = train_data.map(tokenize_function, batched=True)\n",
        "test_data = test_data.map(tokenize_function, batched=True)\n",
        "\n",
        "# Load pre-trained model with custom output layer\n",
        "num_labels = 6  # Three classes each for Target and Severity\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Initialize Trainer with MultiLabelTrainer\n",
        "trainer = MultiLabelTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = trainer.predict(test_data)\n",
        "predicted_logits = predictions.predictions\n",
        "\n",
        "# Separate predictions for Target and Severity\n",
        "predicted_target = np.argmax(predicted_logits[:, :3], axis=1)\n",
        "predicted_severity = np.argmax(predicted_logits[:, 3:], axis=1)\n",
        "\n",
        "# Map back to original classes\n",
        "target_inverse_mapping = {v: k for k, v in target_mapping.items()}\n",
        "severity_inverse_mapping = {v: k for k, v in severity_mapping.items()}\n",
        "\n",
        "predicted_target_labels = [target_inverse_mapping[t] for t in predicted_target]\n",
        "predicted_severity_labels = [severity_inverse_mapping[s] for s in predicted_severity]\n",
        "\n",
        "# Print classification metrics\n",
        "print(\"\\nTarget Classification Report:\")\n",
        "print(classification_report(\n",
        "    [label[0] for label in test_labels],\n",
        "    predicted_target\n",
        "))\n",
        "print(\"\\nSeverity Classification Report:\")\n",
        "print(classification_report(\n",
        "    [label[1] for label in test_labels],\n",
        "    predicted_severity\n",
        "))\n",
        "\n",
        "# Save results for analysis\n",
        "results_df = pd.DataFrame({\n",
        "    \"Id\": val_set['Id'],\n",
        "    \"Tweet\": val_set['Tweet'],\n",
        "    \"Actual_Target\": [target_inverse_mapping[label[0]] for label in test_labels],\n",
        "    \"Actual_Severity\": [severity_inverse_mapping[label[1]] for label in test_labels],\n",
        "    \"Predicted_Target\": predicted_target_labels,\n",
        "    \"Predicted_Severity\": predicted_severity_labels\n",
        "})\n",
        "results_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/shared_task/task_b_val_predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "nEgxbxbuGBHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract text and labels from the test set\n",
        "test_texts1 = test_set['Tweet'].tolist()\n",
        "#test_labels1 = test_set[['Target', 'Severity']].values\n",
        "\n",
        "# Encode labels for the test set\n",
        "#test_labels = [[target_mapping[t], severity_mapping[s]] for t, s in test_labels]\n",
        "\n",
        "# Convert test set to Hugging Face Dataset\n",
        "test_data1 = Dataset.from_dict({'text': test_texts1})\n",
        "\n",
        "# Tokenize the test data\n",
        "test_data1 = test_data1.map(tokenize_function, batched=True)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_predictions = trainer.predict(test_data1)\n",
        "test_predicted_logits = test_predictions.predictions\n",
        "\n",
        "# Separate predictions for Target and Severity\n",
        "test_predicted_target = np.argmax(test_predicted_logits[:, :3], axis=1)\n",
        "test_predicted_severity = np.argmax(test_predicted_logits[:, 3:], axis=1)\n",
        "\n",
        "# Map back to original classes\n",
        "test_predicted_target_labels = [target_inverse_mapping[t] for t in test_predicted_target]\n",
        "test_predicted_severity_labels = [severity_inverse_mapping[s] for s in test_predicted_severity]\n",
        "\n",
        "\n",
        "\n",
        "# Save test set predictions for analysis\n",
        "test_results_df = pd.DataFrame({\n",
        "    \"Id\": test_set['Id'],\n",
        "    \"Tweet\": test_set['Tweet'],\n",
        "    #\"Actual_Target\": [target_inverse_mapping[label[0]] for label in test_labels],\n",
        "    #\"Actual_Severity\": [severity_inverse_mapping[label[1]] for label in test_labels],\n",
        "    \"Predicted_Target\": test_predicted_target_labels,\n",
        "    \"Predicted_Severity\": test_predicted_severity_labels\n",
        "})\n",
        "test_results_df.to_excel(\"/content/drive/MyDrive/Colab Notebooks/shared_task/task_b_testing_predictions.xlsx\", index=False)\n",
        "\n",
        "print(\"Test set evaluation completed. Results saved to 'task_b_test_predictions.xlsx'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f8d0e2a00c474b93976a2658ed7eb833",
            "6a386b66b3d44a3597178e089c1f3b4c",
            "5e1f11b8bd44497fa5891acc24fb9d48",
            "46561e56d2f04f9f99dee7a96384d822",
            "b7f5172c9ddc4bd6acf320f6d8a31e1f",
            "ceb8710bf53943f78fa842923463e4f8",
            "196c91c5a00a4fca8e5653dd0272ab26",
            "e6e6bf0738ab4737a64bdb770a84e811",
            "fccc4a42ab904817b8504ffb698172df",
            "f5866f3701d14425b116b772e16b039d",
            "b14f5a77a6244dd7906be7d524586b60"
          ]
        },
        "id": "ha7B_yC0JfBG",
        "outputId": "4b09f3e0-e8c2-44fd-f7b1-cc110af7fdb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/504 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8d0e2a00c474b93976a2658ed7eb833"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set evaluation completed. Results saved to 'task_b_test_predictions.xlsx'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the two Excel files\n",
        "file1 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/shared_task/val_filtered_data.xlsx')\n",
        "file2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/shared_task/task_b_val_predictions.csv')\n",
        "\n",
        "# Concatenate the two files vertically (by rows)\n",
        "combined_df = pd.concat([file1, file2], axis=0, ignore_index=True)\n",
        "\n",
        "# Optionally, you can drop any duplicates if necessary\n",
        "# combined_df = combined_df.drop_duplicates()\n",
        "combined_df  =combined_df.fillna('N/A')\n",
        "combined_df = combined_df.sort_values(by='Id', ascending=True)\n",
        "# Save the combined dataframe to a new Excel file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Colab Notebooks/shared_task/validation_task_b_predictions_final.csv', index=False)\n",
        "\n",
        "print(\"Files combined successfully and saved as 'val_b_predictions.xlsx'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2BBt0RGDMz-",
        "outputId": "d5ed3e61-af81-4230-889f-f89d855a09b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files combined successfully and saved as 'val_b_predictions.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the two Excel files\n",
        "file1 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/shared_task/test_filtered_data.xlsx')\n",
        "file2 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/shared_task/task_b_testing_predictions.xlsx')\n",
        "\n",
        "# Concatenate the two files vertically (by rows)\n",
        "combined_df = pd.concat([file1, file2], axis=0, ignore_index=True)\n",
        "\n",
        "# Optionally, you can drop any duplicates if necessary\n",
        "# combined_df = combined_df.drop_duplicates()\n",
        "combined_df  = combined_df.fillna('N/A')\n",
        "# Save the combined dataframe to a new Excel file\n",
        "combined_df.to_csv('/content/drive/MyDrive/Colab Notebooks/shared_task/testing_task_b_predictions_final.csv', index=False)\n",
        "\n",
        "print(\"Files combined successfully and saved as 'val_b_predictions.xlsx'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1zKrERwL4OE",
        "outputId": "2eab16e2-19d0-4504-f336-f73455f01ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files combined successfully and saved as 'val_b_predictions.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "input_file_path = \"/content/drive/MyDrive/Colab Notebooks/shared_task/testing_task_b_predictions_final.csv\"  # Replace with your input file path\n",
        "output_file_path = \"/content/drive/MyDrive/Colab Notebooks/shared_task/DCST_Unigoa_Test_TaskB_run.csv\"  # Replace with your desired output file path\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(input_file_path)\n",
        "\n",
        "# Drop the two columns (replace 'Column1' and 'Column2' with actual column names)\n",
        "columns_to_remove = ['Actual_Hate', 'Actual_Fake', 'Predicted_Hate', 'Predicted_Fake']  # Replace with the column names you want to delete\n",
        "df = df.drop(columns=columns_to_remove, errors='ignore')\n",
        "\n",
        "# Sort the DataFrame in ascending order based on the 'Sid' column\n",
        "df = df.sort_values(by='Id', ascending=True)\n",
        "df  =df.fillna('N/A')\n",
        "\n",
        "# Save the modified DataFrame back to a CSV file\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Confirmation message\n",
        "print(f\"Columns removed and data sorted by 'Sid'. Updated file saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyMyPuu1PdTO",
        "outputId": "88fdb2d2-3a07-46d5-b10a-6d94f18c174e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns removed and data sorted by 'Sid'. Updated file saved to /content/drive/MyDrive/Colab Notebooks/shared_task/DCST_Unigoa_Test_TaskB_run.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AulczuqDcrw"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8d0e2a00c474b93976a2658ed7eb833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a386b66b3d44a3597178e089c1f3b4c",
              "IPY_MODEL_5e1f11b8bd44497fa5891acc24fb9d48",
              "IPY_MODEL_46561e56d2f04f9f99dee7a96384d822"
            ],
            "layout": "IPY_MODEL_b7f5172c9ddc4bd6acf320f6d8a31e1f"
          }
        },
        "6a386b66b3d44a3597178e089c1f3b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb8710bf53943f78fa842923463e4f8",
            "placeholder": "​",
            "style": "IPY_MODEL_196c91c5a00a4fca8e5653dd0272ab26",
            "value": "Map: 100%"
          }
        },
        "5e1f11b8bd44497fa5891acc24fb9d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e6bf0738ab4737a64bdb770a84e811",
            "max": 504,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fccc4a42ab904817b8504ffb698172df",
            "value": 504
          }
        },
        "46561e56d2f04f9f99dee7a96384d822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5866f3701d14425b116b772e16b039d",
            "placeholder": "​",
            "style": "IPY_MODEL_b14f5a77a6244dd7906be7d524586b60",
            "value": " 504/504 [00:00&lt;00:00, 4435.06 examples/s]"
          }
        },
        "b7f5172c9ddc4bd6acf320f6d8a31e1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb8710bf53943f78fa842923463e4f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196c91c5a00a4fca8e5653dd0272ab26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6e6bf0738ab4737a64bdb770a84e811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fccc4a42ab904817b8504ffb698172df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5866f3701d14425b116b772e16b039d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14f5a77a6244dd7906be7d524586b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}